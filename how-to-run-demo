# you have all seen the demo and how I run it

# PREPARE FOR ThE DEMO
#

To be done once before summit:
CHECK:
 - neutron ippool size on the public_lan to see
   if it's big enough vs how many instances you want to run
 - nova instances quota has to be double the size of
   the amount of instances you want to run
 - upload a RHEL image
 - calibrate the number of instances you want to show vs
   hw. This will need some test-try-fail-repeat.
   we don't know yet how much this evacuation code scales
   and we only tested 7/10 instances on our end.

To be done in between every demo:

- make the infrastructure is clean, start fresh for now

# HOW TO RESET THE DEMO SETUP:

1) delete all instances and associated floating IPs

I use some thing like this:

netbase=76
for i in $(seq -w 1 7); do
 nova delete test-$i
 sleep 2
 ip=$((netbase + i))
 nova floating-ip-delete 10.16.144.$ip
 sleep 2
done

and verify with:

nova floating-ip-list
nova list

2) stop the cluster

pcs cluster stop --all

wait for the cluster to stop everywhere

reboot the compute nodes and wait that they are fully back online
reboot the controller nodes

This will bring everything back online clean

3) BE AWARE OF https://bugzilla.redhat.com/show_bug.cgi?id=1230485

From time to time, qemu will refresh an internal image and that
refresh will cause nova-compute to block.

During that refresh compute node will be mark "down" and instances
can fail to schedule.

An easy workaround is to create enough instances to run all compute nodes
(use the demo_status.sh to check that) and then delete them again using
the same commands below.

It is wise to do it _before_ running every demo to workaround the bug
and at the same time testing that the environment is back online properly

# RUN THE DEMO

1) make sure pcs status doesn't show any warning or error before you start

2) explain the architecture a bit, 3 controller nodes, 3 compute nodes blabla
   yada yada

3) start demo_status.sh script

# REMEMBER TO FIX THE PATH TO keystonerc_admin in the script

4) create the instances

# YOU MUST REPLACE BOTH THE net-id and the image name.
# The net-id come from neutron net-list, private_lan ID.
# Image name should be obvious

netbase=76
for i in $(seq -w 1 7); do
 nova boot --flavor m1.tiny --image cirros --nic net-id=c0e2b12c-3066-44ee-830c-278d0c3f26ec test-$i
 nova floating-ip-create public_lan
 sleep 30
 ip=$((netbase + i))
 nova floating-ip-associate test-$i 10.16.144.$ip
done

# those snippet makes sure that Instance test-1 is always associated
# with the first public_ip. It makes it easier to run the ping
# test by knowing exactly which ip is with which instance on which
# compute node and kill the right compute node

5) simulate compute node crash

if you can run the ping test
start the ping
identify compute node where instance is running
crash the kernel / pull the network cable for the node
* wait and emjoy the fireworks *

CATCH(es):

* ONE and only ONE at a time. Crash one, see the whole process,
  re-add it to the cluster (see below), the crash the next.

* depending on how fast the node reboots, it might or might
  not be re-added to the cluster.
  if NOT then issue "pcs resource clean cmp1" or cmp2 or cmp3
  IF the update process has been done including point 11, this
  will never happen and node will rejoin the cluster automatically
  within a minute or two after the node has booted

* if the compute node is not fenced within 120 seconds,
  you are probably hitting a know bug with fence and pacemaker_remoted.
  Issue a "pcs resource clean cmp1" or cmp2 or cmp3 and things
  should clear out themselves (make sure to clear it before the node
  reboots and it's online)
  It is a good idea to keep a console open to break the boot process
  if the node come back online before fencing kicks in.

6) simulate controller node crash

Crash one of the nodes, see the node being fenced, rejoin
the cluster et all.

You can run the ping test to show case network not being interrupted
but it is a bit flacky due to a neutron bug, so be careful not to 
overdue it.

7) done
